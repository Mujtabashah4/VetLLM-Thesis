% ============================================
% LLM-Based Fine-Tuning Methodology Section
% Add this to methodology.tex after the original architecture section
% ============================================

\section{Large Language Model Fine-Tuning Approach}
\label{sec:llm_finetuning}

Following the initial deep learning architecture exploration, we adopted a Large Language Model (LLM) fine-tuning approach to leverage pre-trained language understanding capabilities for veterinary diagnosis prediction. This section describes our LLM-based methodology.

\subsection{Model Selection}

We evaluated two state-of-the-art 7B parameter language models for fine-tuning:

\textbf{1. Alpaca-7B (LLaMA-7B based)}:
\begin{itemize}
    \item Base Architecture: LLaMA-7B (Meta AI)
    \item Total Parameters: 6.75 billion
    \item Pre-training: Instruction-tuned on Alpaca dataset
    \item Rationale: Strong general language understanding, efficient inference
\end{itemize}

\textbf{2. QWEN 2.5-7B-Instruct}:
\begin{itemize}
    \item Base Architecture: Qwen2.5-7B-Instruct (Alibaba Cloud)
    \item Total Parameters: ~7 billion
    \item Pre-training: Instruction-tuned with medical/veterinary domain exposure
    \item Rationale: Enhanced reasoning capabilities, better structured output
\end{itemize}

Both models are comparable in size (7B parameters) ensuring fair comparison.

\subsection{Parameter-Efficient Fine-Tuning: LoRA}

To enable fine-tuning on consumer-grade hardware (NVIDIA RTX 4090, 24GB VRAM), we employed \textbf{LoRA (Low-Rank Adaptation)} \cite{hu2021lora}, a parameter-efficient fine-tuning technique.

\subsubsection{LoRA Configuration}

Both models used identical LoRA configurations:

\begin{table}[htbp]
\centering
\caption{LoRA Configuration for Both Models}
\label{tab:lora_config}
\begin{tabular}{lc}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
LoRA Rank (r) & 16 \\
LoRA Alpha ($\alpha$) & 32 \\
LoRA Dropout & 0.05--0.1 \\
Task Type & CAUSAL\_LM \\
Bias & none \\
Target Modules & q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj, down\_proj \\
\bottomrule
\end{tabular}
\end{table}

\textbf{LoRA Principle}: Instead of updating all model parameters, LoRA learns low-rank decomposition matrices:
\begin{equation}
W' = W + \Delta W = W + BA
\end{equation}
where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ with rank $r \ll \min(d,k)$.

\textbf{Benefits}:
\begin{itemize}
    \item Trainable parameters: ~16.7M (0.25\% of total)
    \item Adapter size: ~67 MB (vs 13 GB full model)
    \item Memory efficiency: 99.5\% reduction
    \item Training time: 10--11 minutes (vs hours for full fine-tuning)
\end{itemize}

\subsubsection{Quantization: QLoRA}

For Alpaca-7B, we employed \textbf{4-bit quantization (QLoRA)} \cite{dettmers2023qlora} to further reduce memory requirements:

\begin{itemize}
    \item Quantization Type: NF4 (NormalFloat4)
    \item Compute Dtype: bfloat16
    \item Double Quantization: Enabled
    \item Memory Usage: ~7.7 GB (vs ~18.5 GB full precision)
\end{itemize}

QWEN 2.5-7B was trained in full precision (bfloat16) as sufficient VRAM was available.

\subsection{Training Configuration}

\subsubsection{Hyperparameters}

Table~\ref{tab:training_hyperparams} presents the training hyperparameters:

\begin{table}[htbp]
\centering
\caption{Training Hyperparameters}
\label{tab:training_hyperparams}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Alpaca-7B} & \textbf{QWEN 2.5-7B} \\
\midrule
Max Sequence Length & 512 & 512 \\
Learning Rate & 2.0e-4 & 1.0e-4 \\
Weight Decay & 0.01 & 0.01 \\
Warmup Ratio & 0.03 & 0.03 \\
LR Scheduler & cosine & cosine \\
Batch Size (per device) & 2 & 2 \\
Gradient Accumulation & 4 & 8 \\
Effective Batch Size & 8 & 16 \\
Epochs & 3 & 7 (with early stopping) \\
Optimizer & adamw\_8bit & adamw\_torch \\
Precision & bfloat16 + 4-bit & bfloat16 (full) \\
Gradient Checkpointing & Enabled & Enabled \\
Max Grad Norm & 1.0 & 1.0 \\
Random Seed & 42 & 42 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Note}: Learning rate differences are model-specific optimal values (standard practice). Early stopping was used for QWEN to prevent overfitting (patience=3, threshold=0.001).

\subsubsection{Data Format}

Clinical cases were formatted as instruction-following prompts:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\small]
<|im_start|>system
You are VetLLM, a veterinary clinical assistant...
<|im_end|>
<|im_start|>user
Analyze this veterinary case:
**Species**: Sheep
**Clinical Signs**: fever, labial vesicles, nasal discharge, bloody diarrhea
Provide your diagnosis, differentials, treatment plan, and reasoning.
<|im_end|>
<|im_start|>assistant
1. **Primary Diagnosis**: **Peste des Petits Ruminants** (SNOMED-CT: 1679004)
2. **Differential Diagnoses**: ...
3. **Recommended Treatment**: ...
4. **Clinical Reasoning**: ...
<|im_end|>
\end{lstlisting}

\subsection{Training Procedure}

\subsubsection{Training Process}

\begin{enumerate}
    \item \textbf{Data Loading}: Loaded 373 training samples, 80 validation samples
    \item \textbf{Tokenization}: Applied model-specific tokenizers (Alpaca: SentencePiece, QWEN: tiktoken)
    \item \textbf{Training Loop}: 
    \begin{itemize}
        \item Forward pass through base model + LoRA adapters
        \item Loss computation (causal language modeling)
        \item Backward pass (only LoRA parameters updated)
        \item Gradient accumulation for effective batch size
    \end{itemize}
    \item \textbf{Evaluation}: Validation loss computed every epoch
    \item \textbf{Model Selection}: Best model selected based on validation loss
\end{enumerate}

\subsubsection{Training Results}

\textbf{Alpaca-7B}:
\begin{itemize}
    \item Initial Loss: 3.3359
    \item Final Loss: 0.0533
    \item Loss Reduction: 93.2\%
    \item Training Time: 10 minutes 26 seconds
    \item Best Model: Epoch 3 (final epoch)
\end{itemize}

\textbf{QWEN 2.5-7B}:
\begin{itemize}
    \item Initial Loss: 2.9597
    \item Final Loss: 0.2474
    \item Best Validation Loss: 0.0373 (at Epoch 5)
    \item Loss Reduction: 91.5\%
    \item Training Time: 10 minutes 49 seconds
    \item Best Model: Epoch 5 (selected via early stopping)
\end{itemize}

\subsection{Evaluation Methodology}

\subsubsection{Evaluation Protocol}

To ensure fair comparison, both models were evaluated using \textbf{identical methodology}:

\begin{enumerate}
    \item \textbf{Same Test Set}: Both models evaluated on identical 80-sample test set
    \item \textbf{Same Metrics}: Accuracy, Precision, Recall, F1 (Macro/Micro/Weighted)
    \item \textbf{Same Extraction Logic}: Identical disease name and SNOMED code extraction
    \item \textbf{Same Normalization}: Same disease name normalization rules
    \item \textbf{Same Hardware}: Same GPU (RTX 4090) and software environment
\end{enumerate}

\subsubsection{Evaluation Metrics}

\textbf{Disease Classification Metrics}:
\begin{itemize}
    \item \textbf{Accuracy}: Overall correct predictions / total samples
    \item \textbf{Precision (Macro)}: Average precision across all disease classes
    \item \textbf{Recall (Macro)}: Average recall across all disease classes
    \item \textbf{F1 Score (Macro)}: Harmonic mean of macro precision and recall
    \item \textbf{F1 Score (Micro)}: F1 calculated on all samples together
    \item \textbf{F1 Score (Weighted)}: F1 weighted by class frequency
\end{itemize}

\textbf{SNOMED Code Metrics}:
\begin{itemize}
    \item \textbf{SNOMED Code Accuracy}: Exact match of predicted codes
    \item \textbf{Code Extraction}: Regex-based extraction from model output
\end{itemize}

\subsubsection{Fair Comparison Verification}

We verified that both models were assessed on the \textbf{same ground}:

\begin{table}[htbp]
\centering
\caption{Fair Comparison Verification}
\label{tab:fair_comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Aspect} & \textbf{Alpaca-7B} & \textbf{QWEN 2.5-7B} \\
\midrule
Training Samples & 373 & 373 \\
Validation Samples & 80 & 80 \\
Test Samples & 80 & 80 \\
Data Split Ratio & 70/15/15 & 70/15/15 \\
Random Seed & 42 & 42 \\
LoRA Rank & 16 & 16 \\
LoRA Alpha & 32 & 32 \\
Max Sequence Length & 512 & 512 \\
Evaluation Metrics & Same & Same \\
Evaluation Protocol & Same & Same \\
Hardware & RTX 4090 & RTX 4090 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Conclusion}: Both models were trained and evaluated using identical methodologies, ensuring scientifically valid comparison.

\subsection{Challenges and Solutions}

\subsubsection{Challenge 1: Class Imbalance}

\textbf{Problem}: Severe class imbalance in training data (122:1 ratio between most and least common diseases).

\textbf{Impact}: 
\begin{itemize}
    \item Model over-predicts common diseases (PPR, FMD, Mastitis)
    \item Rare diseases achieve 0\% accuracy
    \item F1 Macro score dragged down by rare disease failures
\end{itemize}

\textbf{Solution Implemented}:
\begin{itemize}
    \item Identified imbalance through data analysis
    \item Documented as limitation for future work
    \item Recommended data augmentation and class-weighted loss
\end{itemize}

\subsubsection{Challenge 2: Memory Constraints}

\textbf{Problem}: Full precision training requires ~18.5 GB VRAM (exceeds RTX 4090 capacity for some models).

\textbf{Solution}:
\begin{itemize}
    \item Alpaca: Used 4-bit quantization (QLoRA) - 7.7 GB usage
    \item QWEN: Full precision training - 18.5 GB usage (within limits)
    \item Gradient checkpointing enabled for both
\end{itemize}

\subsubsection{Challenge 3: Overfitting}

\textbf{Problem}: Training loss decreasing but validation loss plateauing/increasing.

\textbf{Solution}:
\begin{itemize}
    \item Implemented early stopping (patience=3, threshold=0.001)
    \item Selected best model based on validation loss
    \item QWEN: Best model at Epoch 5 (vs final Epoch 7)
\end{itemize}

\subsubsection{Challenge 4: SNOMED Code Extraction}

\textbf{Problem}: Model outputs natural language; SNOMED codes need extraction.

\textbf{Solution}:
\begin{itemize}
    \item Regex-based extraction from model output
    \item Pattern matching for 7-8 digit codes
    \item Validation against known SNOMED code mappings
\end{itemize}

\subsection{Implementation Details}

\subsubsection{Software Stack}

\begin{itemize}
    \item \textbf{PyTorch}: 2.0+
    \item \textbf{Transformers}: Hugging Face library
    \item \textbf{PEFT}: Parameter-Efficient Fine-Tuning library
    \item \textbf{BitsAndBytes}: 4-bit quantization support
    \item \textbf{Accelerate}: Multi-GPU and mixed precision support
\end{itemize}

\subsubsection{Reproducibility}

All experiments are fully reproducible:
\begin{itemize}
    \item Random seeds fixed at 42
    \item Data splits saved and verified
    \item Training configurations saved in YAML files
    \item All code version-controlled
\end{itemize}

