# VetLLM: Veterinary Diagnosis Prediction using LLMs

**Fine-tuning Large Language Models for SNOMED-CT Code Prediction from Veterinary Clinical Notes**

---

## ğŸ“‹ Project Overview

VetLLM is a research project that fine-tunes large language models (LLMs) to predict SNOMED-CT diagnosis codes from veterinary clinical notes. The project has successfully trained and evaluated two models: **Alpaca-7B** and **QWEN 2.5-7B**, with plans to test **Llama3.1-8B**.

---

## ğŸ¯ Key Results

### Models Tested
- âœ… **Alpaca-7B**: 40% accuracy, 46.15% F1 macro, better rare disease handling
- âœ… **QWEN 2.5-7B**: 50% accuracy, 16.44% F1 macro, excellent on common diseases
- â³ **Llama3.1-8B**: Planned for future testing

### Dataset
- **Training**: 373 samples
- **Validation**: 80 samples
- **Test**: 80 samples
- **Total**: 533 unique cases from UVAS DLO System

---

## ğŸ“ Project Structure

```
VetLLM-Thesis/
â”œâ”€â”€ models/              # Base models and fine-tuned checkpoints
â”‚   â”œâ”€â”€ alpaca-7b/       # Alpaca base model
â”‚   â”œâ”€â”€ alpaca-7b-native/ # Alpaca native model
â”‚   â”œâ”€â”€ qwen2.5-7b-instruct/ # QWEN base model
â”‚   â””â”€â”€ vetllm-finetuned*/ # Fine-tuned models
â”‚
â”œâ”€â”€ experiments/          # Training experiments and results
â”‚   â”œâ”€â”€ qwen2.5-7b/      # QWEN experiments
â”‚   â”œâ”€â”€ llama3.1-8b/     # Llama3.1 experiments (future)
â”‚   â”œâ”€â”€ checkpoints/      # Training checkpoints
â”‚   â””â”€â”€ shared/          # Shared training/evaluation code
â”‚
â”œâ”€â”€ reports/              # All reports and documentation
â”‚   â”œâ”€â”€ alpaca/          # Alpaca-specific reports
â”‚   â”œâ”€â”€ qwen/            # QWEN-specific reports
â”‚   â”œâ”€â”€ llama3.1/        # Llama3.1 reports (future)
â”‚   â”œâ”€â”€ comparison/      # Model comparison reports
â”‚   â”œâ”€â”€ general/         # General project reports
â”‚   â””â”€â”€ REPORT_INDEX.md  # Complete documentation index
â”‚
â”œâ”€â”€ logs/                 # Training and evaluation logs
â”‚   â”œâ”€â”€ alpaca/          # Alpaca logs
â”‚   â”œâ”€â”€ qwen/            # QWEN logs
â”‚   â””â”€â”€ general/        # General logs
â”‚
â”œâ”€â”€ data/                 # Data files
â”‚   â””â”€â”€ snomed_codes.json # SNOMED code mappings
â”‚
â”œâ”€â”€ processed_data/       # Processed datasets
â”‚   â”œâ”€â”€ all_processed_data.json
â”‚   â””â”€â”€ Verified_DLO_data_*.json
â”‚
â”œâ”€â”€ scripts/              # Utility scripts (organized by purpose)
â”‚   â”œâ”€â”€ training/        # Training scripts
â”‚   â”œâ”€â”€ evaluation/      # Evaluation scripts
â”‚   â”œâ”€â”€ utils/           # Utility scripts
â”‚   â””â”€â”€ [other scripts]  # Other utilities
â”‚
â”œâ”€â”€ configs/              # Configuration files
â”‚   â”œâ”€â”€ training_config.yaml
â”‚   â””â”€â”€ logging_config.yaml
â”‚
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ QUICK_START.md
â”‚   â””â”€â”€ *.md
â”‚
â”œâ”€â”€ thesis/                # Thesis LaTeX files
â”‚   â”œâ”€â”€ thesis_main.tex
â”‚   â””â”€â”€ chap*/            # Chapter files
â”‚
â””â”€â”€ notebooks/             # Jupyter notebooks
    â””â”€â”€ *.ipynb
```

---

## ğŸ“š Documentation

### Quick Start
1. **Read Reports**: Start with `reports/REPORT_INDEX.md` for complete documentation
2. **Project Summary**: See `reports/general/PROJECT_SUMMARY.md`
3. **Model Reports**: 
   - Alpaca: `reports/alpaca/ALPACA_COMPLETE_REPORT.md`
   - QWEN: `reports/qwen/QWEN_COMPLETE_REPORT.md`
4. **Comparison**: `reports/comparison/COMPREHENSIVE_MODEL_COMPARISON_REPORT.md`

### Key Documents
- **Report Index**: `reports/REPORT_INDEX.md` - Complete guide to all reports
- **Project Summary**: `reports/general/PROJECT_SUMMARY.md` - Project overview
- **Fair Comparison**: `reports/comparison/FAIR_COMPARISON_METHODOLOGY_REPORT.md` - Methodology
- **Root Cause**: `reports/general/ROOT_CAUSE_ANALYSIS.md` - Analysis of limitations
- **Improvement Plan**: `reports/general/IMPROVEMENT_PLAN.md` - Future improvements

---

## ğŸ”¬ Models

### Alpaca-7B
- **Base Model**: LLaMA-7B (Alpaca)
- **Method**: QLoRA (4-bit quantization)
- **Epochs**: 3
- **Performance**: 40% accuracy, 46.15% F1 macro
- **Report**: `reports/alpaca/ALPACA_COMPLETE_REPORT.md`

### QWEN 2.5-7B
- **Base Model**: Qwen2.5-7B-Instruct
- **Method**: LoRA (full precision)
- **Epochs**: 5
- **Performance**: 50% accuracy, 16.44% F1 macro
- **Report**: `reports/qwen/QWEN_COMPLETE_REPORT.md`

### Llama3.1-8B
- **Status**: Planned
- **Directory**: `experiments/llama3.1-8b/`

---

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8+
- CUDA-capable GPU (RTX 4090 tested)
- PyTorch
- Transformers library

### Installation
```bash
# Install dependencies
pip install -r requirements.txt

# Setup environment
bash setup.sh
```

### Training
```bash
# Train QWEN model
python finetune_qwen.py

# Train Alpaca model
python scripts/train_vetllm.py
```

### Evaluation
```bash
# Evaluate QWEN
python evaluate_qwen_comprehensive.py

# Compare models
python compare_models.py
```

---

## ğŸ“Š Results Summary

### Alpaca-7B
- âœ… Better rare disease handling
- âœ… More balanced F1 scores
- âœ… Memory efficient (4-bit quantization)

### QWEN 2.5-7B
- âœ… Higher overall accuracy
- âœ… Excellent on common diseases (PPR: 90.9%)
- âœ… Better validation loss

### Common Limitations
- âš ï¸ Class imbalance affects rare disease performance
- âš ï¸ SNOMED code accuracy needs improvement (33-35%)

---

## ğŸ”§ Future Work

1. **Llama3.1-8B Training**: Fine-tune using same methodology
2. **Data Augmentation**: Generate examples for rare diseases
3. **Class-Weighted Training**: Address class imbalance
4. **Extended Evaluation**: Comprehensive three-way comparison

---

## ğŸ“ Citation

If you use this work, please cite:
```
VetLLM: Fine-tuning Large Language Models for Veterinary Diagnosis Prediction
[Your citation details]
```

---

## ğŸ“„ License

[Your license information]

---

## ğŸ‘¥ Contributors

[Your contributors]

---

**Last Updated**: 2026-01-05  
**Status**: Core Research Complete âœ…

