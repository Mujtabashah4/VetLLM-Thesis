# VetLLM Experiments

Fine-tuning Large Language Models for Veterinary Clinical Diagnosis

---

## ğŸ¯ Overview

This directory contains the complete experimental framework for fine-tuning LLMs on veterinary clinical data for disease diagnosis prediction in livestock (Cow, Buffalo, Sheep, Goat).

### Models

| Model | Role | Status |
|-------|------|--------|
| **Llama 3.1 8B Instruct** | Primary baseline | âœ… Ready |
| **Qwen2.5 7B Instruct** | Secondary comparison | âœ… Ready |

### Dataset Statistics

| Metric | Value |
|--------|-------|
| Raw clinical records | 1,602 |
| **Unique cases (after deduplication)** | **533** |
| Training samples | 373 (70%) |
| Validation samples | 80 (15%) |
| Test samples | 80 (15%) |
| Diseases covered | 22 unique conditions |
| Species | Cow, Buffalo, Sheep, Goat |

> **Note**: 1,069 duplicate entries (67%) were removed to prevent data leakage between train/test splits. See [EXPERIMENTAL_PLAN.md](EXPERIMENTAL_PLAN.md) for details.

---

## ğŸ“ Directory Structure

```
experiments/
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ EXPERIMENTAL_PLAN.md          # Detailed experimental methodology
â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚
â”œâ”€â”€ shared/                       # Shared utilities
â”‚   â”œâ”€â”€ data_preprocessor.py      # Data preprocessing pipeline
â”‚   â”œâ”€â”€ train.py                  # Unified training script
â”‚   â”œâ”€â”€ inference.py              # Inference utilities
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ evaluate.py           # Evaluation pipeline
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ llama3.1-8b/                  # Llama 3.1 experiment (PRIMARY)
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ training_config.yaml  # Training configuration
â”‚   â”œâ”€â”€ data/                     # Preprocessed data âœ… READY
â”‚   â”‚   â”œâ”€â”€ train.json            # 373 unique samples
â”‚   â”‚   â”œâ”€â”€ validation.json       # 80 unique samples
â”‚   â”‚   â”œâ”€â”€ test.json             # 80 unique samples
â”‚   â”‚   â””â”€â”€ dataset_stats.json    # Disease/species distribution
â”‚   â”œâ”€â”€ checkpoints/              # Model checkpoints (after training)
â”‚   â”œâ”€â”€ results/                  # Evaluation results
â”‚   â”œâ”€â”€ logs/                     # Training logs
â”‚   â””â”€â”€ run_experiment.sh         # One-click runner
â”‚
â””â”€â”€ qwen2.5-7b/                   # Qwen2.5 experiment (SECONDARY)
    â”œâ”€â”€ configs/
    â”‚   â””â”€â”€ training_config.yaml
    â”œâ”€â”€ data/                     # Preprocessed data âœ… READY
    â”‚   â”œâ”€â”€ train.json            # 373 unique samples
    â”‚   â”œâ”€â”€ validation.json       # 80 unique samples
    â”‚   â”œâ”€â”€ test.json             # 80 unique samples
    â”‚   â””â”€â”€ dataset_stats.json
    â”œâ”€â”€ checkpoints/
    â”œâ”€â”€ results/
    â”œâ”€â”€ logs/
    â””â”€â”€ run_experiment.sh
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r experiments/requirements.txt
```

### 2. Run Llama 3.1 8B Experiment

```bash
cd experiments/llama3.1-8b
./run_experiment.sh all
```

This will:
1. âœ… Preprocess data (already done)
2. Train the model with QLoRA
3. Evaluate on test set
4. Generate comparison report

### 3. Run Qwen2.5 7B Experiment (after Llama)

```bash
cd experiments/qwen2.5-7b
./run_experiment.sh all
```

---

## ğŸ“Š Data Format

### Llama 3.1 Format

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are VetLLM, a veterinary clinical assistant...
<|eot_id|><|start_header_id|>user<|end_header_id|>

Species: Cow
Clinical presentation: fever and diarrhea
...
<|eot_id|><|start_header_id|>assistant<|end_header_id|>

1. **Primary Diagnosis**: **Disease Name** (SNOMED-CT: code)
...
<|eot_id|>
```

### Qwen2.5 Format (ChatML)

```
<|im_start|>system
You are VetLLM, a veterinary clinical assistant...
<|im_end|>
<|im_start|>user
**Species**: Cow
**Clinical Signs**: fever and diarrhea
...
<|im_end|>
<|im_start|>assistant
1. **Primary Diagnosis**: **Disease Name** (SNOMED-CT: code)
...
<|im_end|>
```

---

## ğŸ“ˆ Evaluation Metrics

### Classification
- Accuracy (exact match on diagnosis)
- F1 Score (macro/micro/weighted)
- Precision & Recall

### Generation
- BLEU Score
- ROUGE-1/2/L

---

## ğŸ”§ Manual Commands

### Preprocess Data

```bash
# For Llama 3.1
python experiments/shared/data_preprocessor.py \
    --dataset-dir Dataset_UVAS \
    --output-dir experiments/llama3.1-8b/data \
    --model llama3.1

# For Qwen2.5
python experiments/shared/data_preprocessor.py \
    --dataset-dir Dataset_UVAS \
    --output-dir experiments/qwen2.5-7b/data \
    --model qwen2.5
```

### Train Model

```bash
python experiments/shared/train.py \
    --config experiments/llama3.1-8b/configs/training_config.yaml
```

### Evaluate Model

```bash
python experiments/shared/evaluation/evaluate.py \
    --model-path experiments/llama3.1-8b/checkpoints/final \
    --base-model meta-llama/Llama-3.1-8B-Instruct \
    --test-data experiments/llama3.1-8b/data/test.json \
    --output-dir experiments/llama3.1-8b/results
```

### Interactive Inference

```bash
python experiments/shared/inference.py interactive \
    --model meta-llama/Llama-3.1-8B-Instruct \
    --adapter experiments/llama3.1-8b/checkpoints/final \
    --model-type llama3.1
```

---

## ğŸ’» Hardware Requirements

| Setup | Llama 3.1 8B | Qwen2.5 7B |
|-------|--------------|------------|
| **Min GPU VRAM** | 16 GB | 14 GB |
| **Recommended** | 24 GB | 24 GB |
| **RAM** | 32 GB | 32 GB |

### Supported GPUs
- NVIDIA A100 (40GB/80GB)
- NVIDIA RTX 4090 (24GB)
- NVIDIA RTX 3090 (24GB)
- NVIDIA V100 (32GB)

---

## ğŸ“ Notes

1. **HuggingFace Login Required**: Run `huggingface-cli login` before downloading models

2. **Wandb (Optional)**: Set `report_to: "none"` in config to disable

3. **Memory Issues**: Reduce batch size or enable gradient checkpointing

---

## ğŸ“š Documentation

- [Detailed Experimental Plan](EXPERIMENTAL_PLAN.md)
- [Project Documentation](../docs/README.md)

---

*Last Updated: January 2026*

