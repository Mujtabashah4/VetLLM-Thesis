{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêæ VetLLM: Complete Testing & Fine-Tuning Notebook\n",
    "\n",
    "This notebook provides a comprehensive guide to:\n",
    "1. **Understanding the data format** required for fine-tuning\n",
    "2. **Testing the base model** (zero-shot performance)\n",
    "3. **Fine-tuning the model** with your data\n",
    "4. **Evaluating** the fine-tuned model\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. Environment Setup\n",
    "2. Understanding Data Formats\n",
    "3. Testing Base Model (Zero-Shot)\n",
    "4. Data Preprocessing\n",
    "5. Fine-Tuning with LoRA\n",
    "6. Evaluation & Comparison\n",
    "7. Interactive Demo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install all required dependencies and check our hardware.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 1: Install Dependencies\n",
    "# ============================================\n",
    "# Run this cell to install all required packages\n",
    "!pip install -q torch torchvision torchaudio \\\n",
    "transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0 \\\n",
    "peft>=0.6.0 bitsandbytes>=0.41.0 scikit-learn pandas numpy \\\n",
    "matplotlib seaborn sentencepiece protobuf\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üñ•Ô∏è  HARDWARE CHECK\n",
      "============================================================\n",
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: False\n",
      "Using CPU\n",
      "\n",
      "Using device: cpu\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 2: Import Libraries & Check Hardware\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Check hardware\n",
    "print(\"=\"*60)\n",
    "print(\"üñ•Ô∏è  HARDWARE CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"MPS (Apple Silicon) available\")\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = \"cpu\"\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Understanding Data Formats\n",
    "\n",
    "### üìä VetLLM uses three data formats:\n",
    "\n",
    "1. **Raw Veterinary Notes** - Your original clinical notes\n",
    "2. **Instruction Format** - Formatted for instruction-tuning\n",
    "3. **Alpaca Prompt** - The actual input to the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# UNDERSTANDING DATA FORMAT: Raw Veterinary Notes\n",
    "# ============================================\n",
    "# This is your INPUT format - clinical notes with SNOMED-CT codes\n",
    "\n",
    "raw_veterinary_note_example = {\n",
    "    \"clinical_note\": \"Dog, 4 years old, Golden Retriever. Presents with acute lethargy and decreased appetite. Physical exam: pale gums, mild fever (39.3¬∞C). Owner reports possible exposure to spoiled food.\",\n",
    "    \"snomed_codes\": [\"397983004\", \"79890006\"]  # Lethargy, Loss of appetite\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìã FORMAT 1: Raw Veterinary Note (Your Input Data)\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(raw_veterinary_note_example, indent=2))\n",
    "print(\"\\nüí° This is how you should structure your data file!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# UNDERSTANDING DATA FORMAT: Instruction Format\n",
    "# ============================================\n",
    "# After preprocessing, data is converted to instruction-following format\n",
    "\n",
    "instruction_format_example = {\n",
    "    \"instruction\": \"Analyze the following veterinary clinical note and predict the SNOMED-CT diagnosis codes.\",\n",
    "    \"input\": \"Clinical Note: Dog, 4 years old, Golden Retriever. Presents with acute lethargy and decreased appetite. Physical exam: pale gums, mild fever (39.3¬∞C). Owner reports possible exposure to spoiled food.\",\n",
    "    \"output\": \"Diagnosed conditions: 397983004, 79890006\",\n",
    "    \"snomed_codes\": [\"397983004\", \"79890006\"]\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìã FORMAT 2: Instruction Format (For Training)\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(instruction_format_example, indent=2))\n",
    "print(\"\\nüí° This format is used internally for training!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# UNDERSTANDING DATA FORMAT: Alpaca Prompt\n",
    "# ============================================\n",
    "# This is the actual prompt format sent to the model\n",
    "\n",
    "def create_alpaca_prompt(instruction: str, input_text: str = \"\", output: str = \"\") -> str:\n",
    "    \"\"\"Create Alpaca-style prompt for the model.\"\"\"\n",
    "    if input_text:\n",
    "        prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Create example prompt\n",
    "example_prompt = create_alpaca_prompt(\n",
    "    instruction=instruction_format_example[\"instruction\"],\n",
    "    input_text=instruction_format_example[\"input\"],\n",
    "    output=\"\"  # Empty during inference\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìã FORMAT 3: Alpaca Prompt (Actual Model Input)\")\n",
    "print(\"=\"*60)\n",
    "print(example_prompt)\n",
    "print(\"\\nüí° This is what the model actually sees!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SNOMED-CT Code Reference\n",
    "# ============================================\n",
    "# Common SNOMED-CT codes used in veterinary diagnosis\n",
    "\n",
    "SNOMED_CODES = {\n",
    "    \"397983004\": \"Lethargy\",\n",
    "    \"79890006\": \"Loss of appetite\",\n",
    "    \"422400008\": \"Vomiting\",\n",
    "    \"62315008\": \"Diarrhea\",\n",
    "    \"267036007\": \"Difficulty breathing\",\n",
    "    \"16973004\": \"Lameness\",\n",
    "    \"57676002\": \"Joint pain\",\n",
    "    \"271807003\": \"Skin rash\",\n",
    "    \"424492005\": \"Scratching\",\n",
    "    \"91175000\": \"Seizure\",\n",
    "    \"89362005\": \"Weight loss\",\n",
    "    \"49727002\": \"Cough\",\n",
    "    \"64531003\": \"Nasal discharge\",\n",
    "    \"271860004\": \"Abdominal distension\",\n",
    "    \"25786006\": \"Behavioral changes\",\n",
    "    \"386661006\": \"Fever\",\n",
    "    \"34095006\": \"Dehydration\",\n",
    "    \"387603000\": \"Loss of balance\",\n",
    "    \"17173007\": \"Excessive thirst\",\n",
    "    \"139394000\": \"Difficulty urinating\",\n",
    "    \"246636008\": \"Eye discharge\"\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üè• SNOMED-CT Code Reference\")\n",
    "print(\"=\"*60)\n",
    "for code, description in list(SNOMED_CODES.items())[:10]:\n",
    "    print(f\"  {code}: {description}\")\n",
    "print(f\"  ... and {len(SNOMED_CODES)-10} more codes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Testing Base Model (Zero-Shot)\n",
    "\n",
    "Let's test the **Alpaca-7B base model** WITHOUT any fine-tuning to understand its baseline performance on veterinary diagnosis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION: Choose Your Model\n",
    "# ============================================\n",
    "\n",
    "# Choose one based on your GPU memory:\n",
    "# - \"wxjiao/alpaca-7b\" : Full 7B model (requires 16GB+ GPU)\n",
    "# - \"openlm-research/open_llama_3b\" : Smaller model (requires 8GB+ GPU)\n",
    "# - \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" : Tiny model (works on most GPUs)\n",
    "\n",
    "# For Colab Free Tier (T4 GPU with 15GB), use:\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Change to \"wxjiao/alpaca-7b\" if you have enough memory\n",
    "\n",
    "# For Kaggle/Colab Pro (A100/V100):\n",
    "# MODEL_NAME = \"wxjiao/alpaca-7b\"\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")\n",
    "print(\"\\n‚ö†Ô∏è If you get OOM errors, switch to a smaller model above!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD BASE MODEL & TOKENIZER\n",
    "# ============================================\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "print(\"This may take a few minutes for the first time (downloading model weights).\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# Set pad token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with appropriate settings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Move to device if not using device_map\n",
    "if device != \"cuda\":\n",
    "    model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded successfully!\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INFERENCE FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def predict_diagnosis(\n",
    "    clinical_note: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_new_tokens: int = 100,\n",
    "    temperature: float = 0.1\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate diagnosis prediction for a clinical note.\n",
    "    \n",
    "    Args:\n",
    "        clinical_note: The veterinary clinical note\n",
    "        model: The loaded model\n",
    "        tokenizer: The loaded tokenizer\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (lower = more deterministic)\n",
    "    \n",
    "    Returns:\n",
    "        The model's prediction\n",
    "    \"\"\"\n",
    "    instruction = \"Analyze the following veterinary clinical note and predict the SNOMED-CT diagnosis codes. List the numeric codes that apply.\"\n",
    "    \n",
    "    prompt = create_alpaca_prompt(\n",
    "        instruction=instruction,\n",
    "        input_text=f\"Clinical Note: {clinical_note}\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature if temperature > 0 else None,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ Inference function created!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TEST CASES: Zero-Shot Performance\n",
    "# ============================================\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"clinical_note\": \"Dog, 4 years old, Golden Retriever. Presents with acute lethargy and decreased appetite. Physical exam: pale gums, mild fever (39.3¬∞C).\",\n",
    "        \"expected_codes\": [\"397983004\", \"79890006\"],\n",
    "        \"expected_diagnosis\": \"Lethargy, Loss of appetite\"\n",
    "    },\n",
    "    {\n",
    "        \"clinical_note\": \"Cat, 2 years old, Siamese. Vomiting and diarrhea for 2 days. Dehydrated, temperature 39.8¬∞C.\",\n",
    "        \"expected_codes\": [\"422400008\", \"62315008\", \"34095006\"],\n",
    "        \"expected_diagnosis\": \"Vomiting, Diarrhea, Dehydration\"\n",
    "    },\n",
    "    {\n",
    "        \"clinical_note\": \"Rabbit, 1 year old. Hair loss and itchy skin, some scabs around neck. Eats well and otherwise healthy.\",\n",
    "        \"expected_codes\": [\"271807003\", \"424492005\"],\n",
    "        \"expected_diagnosis\": \"Skin rash, Scratching\"\n",
    "    },\n",
    "    {\n",
    "        \"clinical_note\": \"Horse, 8 years old, Thoroughbred. Limping on front left leg. Joint swelling observed. No fever.\",\n",
    "        \"expected_codes\": [\"16973004\", \"57676002\"],\n",
    "        \"expected_diagnosis\": \"Lameness, Joint pain\"\n",
    "    },\n",
    "    {\n",
    "        \"clinical_note\": \"Dog, 3 years old, Beagle. Sudden onset of seizures. No prior history. Post-ictal confusion.\",\n",
    "        \"expected_codes\": [\"91175000\"],\n",
    "        \"expected_diagnosis\": \"Seizure\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ ZERO-SHOT TESTING (Base Model without Fine-tuning)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTesting model's ability to predict diagnoses without any training...\\n\")\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"\\n{'‚îÄ'*60}\")\n",
    "    print(f\"üìã Test Case {i}\")\n",
    "    print(f\"{'‚îÄ'*60}\")\n",
    "    print(f\"\\nüìù Clinical Note:\")\n",
    "    print(f\"   {test['clinical_note']}\")\n",
    "    print(f\"\\n‚úÖ Expected: {test['expected_diagnosis']}\")\n",
    "    print(f\"   Codes: {test['expected_codes']}\")\n",
    "    \n",
    "    prediction = predict_diagnosis(test['clinical_note'], model, tokenizer)\n",
    "    print(f\"\\nü§ñ Model Prediction:\")\n",
    "    print(f\"   {prediction[:500]}...\" if len(prediction) > 500 else f\"   {prediction}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° Observation: Base model may not give accurate SNOMED codes.\")\n",
    "print(\"   Fine-tuning will significantly improve this performance!\")\n",
    "print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Preprocessing\n",
    "\n",
    "Now let's prepare data for fine-tuning. We'll create synthetic training data and show you how to format your own data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SYNTHETIC DATA GENERATION\n",
    "# ============================================\n",
    "\n",
    "def create_synthetic_veterinary_data(num_samples: int = 100) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create synthetic veterinary training data.\n",
    "    \n",
    "    In practice, you would replace this with your real data!\n",
    "    \"\"\"\n",
    "    \n",
    "    species_options = [\"dog\", \"cat\", \"rabbit\", \"bird\", \"horse\"]\n",
    "    breed_options = {\n",
    "        \"dog\": [\"Labrador\", \"German Shepherd\", \"Golden Retriever\", \"Bulldog\", \"Beagle\", \"Poodle\"],\n",
    "        \"cat\": [\"Persian\", \"Siamese\", \"Maine Coon\", \"British Shorthair\", \"Ragdoll\"],\n",
    "        \"rabbit\": [\"Holland Lop\", \"Netherland Dwarf\", \"Mini Rex\", \"Lionhead\"],\n",
    "        \"bird\": [\"Cockatiel\", \"Budgie\", \"Canary\", \"Parrot\"],\n",
    "        \"horse\": [\"Thoroughbred\", \"Quarter Horse\", \"Arabian\", \"Pinto\"]\n",
    "    }\n",
    "    \n",
    "    # Condition templates with associated codes\n",
    "    conditions = [\n",
    "        {\n",
    "            \"complaint\": \"lethargy and decreased appetite\",\n",
    "            \"findings\": \"mild dehydration, pale mucous membranes\",\n",
    "            \"codes\": [\"397983004\", \"79890006\"]\n",
    "        },\n",
    "        {\n",
    "            \"complaint\": \"vomiting and diarrhea\",\n",
    "            \"findings\": \"dehydration, elevated temperature\",\n",
    "            \"codes\": [\"422400008\", \"62315008\"]\n",
    "        },\n",
    "        {\n",
    "            \"complaint\": \"difficulty breathing\",\n",
    "            \"findings\": \"respiratory distress, abnormal lung sounds\",\n",
    "            \"codes\": [\"267036007\"]\n",
    "        },\n",
    "        {\n",
    "            \"complaint\": \"limping and joint pain\",\n",
    "            \"findings\": \"joint swelling, pain on palpation\",\n",
    "            \"codes\": [\"16973004\", \"57676002\"]\n",
    "        },\n",
    "        {\n",
    "            \"complaint\": \"skin irritation and scratching\",\n",
    "            \"findings\": \"skin lesions, erythema\",\n",
    "            \"codes\": [\"271807003\", \"424492005\"]\n",
    "        },\n",
    "        {\n",
    "            \"complaint\": \"seizure activity\",\n",
    "            \"findings\": \"post-ictal confusion, neurological abnormalities\",\n",
    "            \"codes\": [\"91175000\"]\n",
    "        },\n",
    "        {\n",
    "            \"complaint\": \"weight loss\",\n",
    "            \"findings\": \"muscle wasting, poor body condition\",\n",
    "            \"codes\": [\"89362005\"]\n",
    "        },\n",
    "        {\n",
    "            \"complaint\": \"coughing and nasal discharge\",\n",
    "            \"findings\": \"nasal discharge, enlarged lymph nodes\",\n",
    "            \"codes\": [\"49727002\", \"64531003\"]\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    instructions = [\n",
    "        \"Analyze the following veterinary clinical note and predict the SNOMED-CT diagnosis codes.\",\n",
    "        \"Based on the clinical findings, identify the most likely SNOMED-CT diagnosis codes.\",\n",
    "        \"What are the appropriate SNOMED-CT codes for this veterinary case?\",\n",
    "        \"Determine the diagnosis codes that best match this clinical presentation.\",\n",
    "    ]\n",
    "    \n",
    "    data = []\n",
    "    for i in range(num_samples):\n",
    "        species = random.choice(species_options)\n",
    "        breed = random.choice(breed_options[species])\n",
    "        age = random.randint(1, 15)\n",
    "        condition = random.choice(conditions)\n",
    "        temp = round(random.uniform(37.5, 41.0), 1)\n",
    "        \n",
    "        clinical_note = f\"{species.capitalize()}, {age} years old, {breed}. Presents with {condition['complaint']}. Physical exam: {condition['findings']}. Temperature: {temp}¬∞C.\"\n",
    "        \n",
    "        data.append({\n",
    "            \"instruction\": random.choice(instructions),\n",
    "            \"input\": f\"Clinical Note: {clinical_note}\",\n",
    "            \"output\": f\"Diagnosed conditions: {', '.join(condition['codes'])}\",\n",
    "            \"snomed_codes\": condition['codes']\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Create training and validation data\n",
    "NUM_TRAIN_SAMPLES = 200  # Increase for better results\n",
    "NUM_VAL_SAMPLES = 50\n",
    "\n",
    "print(\"Creating synthetic training data...\")\n",
    "train_data = create_synthetic_veterinary_data(NUM_TRAIN_SAMPLES)\n",
    "val_data = create_synthetic_veterinary_data(NUM_VAL_SAMPLES)\n",
    "\n",
    "print(f\"‚úÖ Created {len(train_data)} training samples\")\n",
    "print(f\"‚úÖ Created {len(val_data)} validation samples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìã Sample training data:\")\n",
    "print(json.dumps(train_data[0], indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# HOW TO USE YOUR OWN DATA\n",
    "# ============================================\n",
    "\n",
    "# If you have your own veterinary notes, format them like this:\n",
    "\n",
    "your_data_template = '''\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Analyze the following veterinary clinical note and predict the SNOMED-CT diagnosis codes.\",\n",
    "        \"input\": \"Clinical Note: [YOUR CLINICAL NOTE HERE]\",\n",
    "        \"output\": \"Diagnosed conditions: [CODE1], [CODE2], ...\",\n",
    "        \"snomed_codes\": [\"CODE1\", \"CODE2\"]\n",
    "    },\n",
    "    // ... more samples\n",
    "]\n",
    "'''\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìÅ HOW TO USE YOUR OWN DATA\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. Create a JSON file with this structure:\")\n",
    "print(your_data_template)\n",
    "print(\"\\n2. Upload to Colab/Kaggle\")\n",
    "print(\"\\n3. Load with:\")\n",
    "print(\"   with open('your_data.json', 'r') as f:\")\n",
    "print(\"       train_data = json.load(f)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREPARE DATASET FOR TRAINING\n",
    "# ============================================\n",
    "\n",
    "def prepare_training_data(data: List[Dict], tokenizer, max_length: int = 512) -> Dataset:\n",
    "    \"\"\"\n",
    "    Convert instruction data to tokenized dataset.\n",
    "    \"\"\"\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        prompt = create_alpaca_prompt(\n",
    "            instruction=item[\"instruction\"],\n",
    "            input_text=item.get(\"input\", \"\"),\n",
    "            output=item[\"output\"]\n",
    "        )\n",
    "        formatted_data.append({\"text\": prompt})\n",
    "    \n",
    "    dataset = Dataset.from_list(formatted_data)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "print(\"Preparing datasets for training...\")\n",
    "train_dataset = prepare_training_data(train_data, tokenizer)\n",
    "val_dataset = prepare_training_data(val_data, tokenizer)\n",
    "\n",
    "print(f\"‚úÖ Training dataset: {len(train_dataset)} samples\")\n",
    "print(f\"‚úÖ Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"\\nSample tokenized length: {len(train_dataset[0]['input_ids'])} tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Fine-Tuning with LoRA\n",
    "\n",
    "Now let's fine-tune the model using **LoRA (Low-Rank Adaptation)** - a memory-efficient technique that only trains a small subset of parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP LORA CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,                          # Rank of the low-rank matrices\n",
    "    lora_alpha=32,                 # Scaling factor\n",
    "    lora_dropout=0.1,              # Dropout probability\n",
    "    target_modules=[               # Modules to apply LoRA to\n",
    "        \"q_proj\", \"v_proj\",        # Attention projections\n",
    "        \"k_proj\", \"o_proj\"         # Key and output projections\n",
    "    ],\n",
    "    bias=\"none\"                    # Don't train biases\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# APPLY LORA TO MODEL\n",
    "# ============================================\n",
    "\n",
    "# Reload model for training (fresh instance)\n",
    "print(\"Reloading model for LoRA training...\")\n",
    "\n",
    "model_for_training = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "if device != \"cuda\":\n",
    "    model_for_training = model_for_training.to(device)\n",
    "\n",
    "# Apply LoRA\n",
    "print(\"Applying LoRA adapters...\")\n",
    "model_for_training = get_peft_model(model_for_training, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model_for_training.print_trainable_parameters()\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model_for_training.gradient_checkpointing_enable()\n",
    "model_for_training.train()\n",
    "\n",
    "print(\"\\n‚úÖ LoRA adapters applied!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "OUTPUT_DIR = \"./vetllm-finetuned\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,                    # Number of epochs\n",
    "    per_device_train_batch_size=4,         # Batch size (reduce if OOM)\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,         # Effective batch = 4 * 4 = 16\n",
    "    \n",
    "    # Learning rate and optimization\n",
    "    learning_rate=2e-4,                    # Learning rate for LoRA\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Memory optimization\n",
    "    fp16=device == \"cuda\",                 # Mixed precision for CUDA\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    report_to=\"none\",                      # Disable wandb for notebook\n",
    "    \n",
    "    # Other\n",
    "    dataloader_drop_last=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# START TRAINING\n",
    "# ============================================\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(\"\\nThis will take a few minutes...\\n\")\n",
    "\n",
    "# Train!\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training steps: {train_result.global_step}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SAVE THE FINE-TUNED MODEL\n",
    "# ============================================\n",
    "\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    print(f\"  - {f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Evaluation & Comparison\n",
    "\n",
    "Let's compare the fine-tuned model with the base model!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD FINE-TUNED MODEL\n",
    "# ============================================\n",
    "\n",
    "print(\"Loading fine-tuned model...\")\n",
    "\n",
    "# The model_for_training already has the fine-tuned weights\n",
    "finetuned_model = model_for_training\n",
    "finetuned_model.eval()\n",
    "\n",
    "print(\"‚úÖ Fine-tuned model ready for evaluation!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARE BASE VS FINE-TUNED MODEL\n",
    "# ============================================\n",
    "\n",
    "def extract_snomed_codes(prediction: str) -> List[str]:\n",
    "    \"\"\"Extract SNOMED-CT codes from model prediction.\"\"\"\n",
    "    codes = re.findall(r'\\b\\d{6,18}\\b', prediction)\n",
    "    return list(dict.fromkeys(codes))[:10]  # Unique codes, max 10\n",
    "\n",
    "def evaluate_prediction(pred_codes: List[str], true_codes: List[str]) -> Dict:\n",
    "    \"\"\"Calculate evaluation metrics.\"\"\"\n",
    "    pred_set = set(pred_codes)\n",
    "    true_set = set(true_codes)\n",
    "    \n",
    "    exact_match = int(pred_set == true_set)\n",
    "    jaccard = len(pred_set & true_set) / len(pred_set | true_set) if pred_set | true_set else 1.0\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": exact_match,\n",
    "        \"jaccard\": jaccard,\n",
    "        \"pred_codes\": pred_codes,\n",
    "        \"true_codes\": true_codes\n",
    "    }\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üî¨ COMPARISON: Base Model vs Fine-Tuned Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_tests = test_cases[:3]  # Use first 3 test cases\n",
    "\n",
    "for i, test in enumerate(comparison_tests, 1):\n",
    "    print(f\"\\n{'‚îÄ'*60}\")\n",
    "    print(f\"üìã Test Case {i}\")\n",
    "    print(f\"{'‚îÄ'*60}\")\n",
    "    print(f\"\\nüìù Clinical Note: {test['clinical_note'][:100]}...\")\n",
    "    print(f\"\\n‚úÖ Expected: {test['expected_codes']}\")\n",
    "    \n",
    "    # Base model prediction\n",
    "    base_pred = predict_diagnosis(test['clinical_note'], model, tokenizer)\n",
    "    base_codes = extract_snomed_codes(base_pred)\n",
    "    base_eval = evaluate_prediction(base_codes, test['expected_codes'])\n",
    "    \n",
    "    print(f\"\\nüîµ Base Model:\")\n",
    "    print(f\"   Prediction: {base_pred[:200]}...\" if len(base_pred) > 200 else f\"   Prediction: {base_pred}\")\n",
    "    print(f\"   Extracted codes: {base_codes}\")\n",
    "    print(f\"   Jaccard: {base_eval['jaccard']:.2f}\")\n",
    "    \n",
    "    # Fine-tuned model prediction\n",
    "    ft_pred = predict_diagnosis(test['clinical_note'], finetuned_model, tokenizer)\n",
    "    ft_codes = extract_snomed_codes(ft_pred)\n",
    "    ft_eval = evaluate_prediction(ft_codes, test['expected_codes'])\n",
    "    \n",
    "    print(f\"\\nüü¢ Fine-Tuned Model:\")\n",
    "    print(f\"   Prediction: {ft_pred[:200]}...\" if len(ft_pred) > 200 else f\"   Prediction: {ft_pred}\")\n",
    "    print(f\"   Extracted codes: {ft_codes}\")\n",
    "    print(f\"   Jaccard: {ft_eval['jaccard']:.2f}\")\n",
    "    \n",
    "    # Improvement\n",
    "    improvement = ft_eval['jaccard'] - base_eval['jaccard']\n",
    "    print(f\"\\nüìà Improvement: {improvement:+.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Interactive Demo\n",
    "\n",
    "Try your own clinical notes!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INTERACTIVE PREDICTION\n",
    "# ============================================\n",
    "\n",
    "def interactive_predict(clinical_note: str):\n",
    "    \"\"\"Get predictions from both models and compare.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"üîÆ DIAGNOSIS PREDICTION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nüìù Input: {clinical_note}\")\n",
    "    \n",
    "    # Base model\n",
    "    print(f\"\\nüîµ Base Model Prediction:\")\n",
    "    base_pred = predict_diagnosis(clinical_note, model, tokenizer)\n",
    "    print(f\"   {base_pred}\")\n",
    "    base_codes = extract_snomed_codes(base_pred)\n",
    "    print(f\"   Codes: {base_codes}\")\n",
    "    \n",
    "    # Fine-tuned model\n",
    "    print(f\"\\nüü¢ Fine-Tuned Model Prediction:\")\n",
    "    ft_pred = predict_diagnosis(clinical_note, finetuned_model, tokenizer)\n",
    "    print(f\"   {ft_pred}\")\n",
    "    ft_codes = extract_snomed_codes(ft_pred)\n",
    "    print(f\"   Codes: {ft_codes}\")\n",
    "    \n",
    "    # Translate codes\n",
    "    print(f\"\\nüìã Code Meanings:\")\n",
    "    for code in ft_codes:\n",
    "        meaning = SNOMED_CODES.get(code, \"Unknown code\")\n",
    "        print(f\"   {code}: {meaning}\")\n",
    "\n",
    "# Example usage\n",
    "interactive_predict(\n",
    "    \"Cat, 5 years old, Persian. Owner reports excessive scratching for 2 weeks. Hair loss around ears and neck. Some redness and scabs visible.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRY YOUR OWN CLINICAL NOTE\n",
    "# ============================================\n",
    "\n",
    "# üëá MODIFY THIS TEXT TO TEST YOUR OWN CLINICAL NOTES!\n",
    "your_clinical_note = \"Dog, 3 years old, German Shepherd. Sudden onset of seizures lasting 2 minutes. First episode. Post-ictal confusion. No prior history of seizures.\"\n",
    "\n",
    "interactive_predict(your_clinical_note)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Data Format for Fine-Tuning:**\n",
    "   ```json\n",
    "   {\n",
    "     \"instruction\": \"Analyze the clinical note...\",\n",
    "     \"input\": \"Clinical Note: [Your note here]\",\n",
    "     \"output\": \"Diagnosed conditions: CODE1, CODE2\",\n",
    "     \"snomed_codes\": [\"CODE1\", \"CODE2\"]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Model Input Format (Alpaca Prompt):**\n",
    "   ```\n",
    "   Below is an instruction that describes a task...\n",
    "   ### Instruction:\n",
    "   [instruction]\n",
    "   ### Input:\n",
    "   [clinical note]\n",
    "   ### Response:\n",
    "   ```\n",
    "\n",
    "3. **Fine-Tuning Improves Performance:**\n",
    "   - Base model: Limited veterinary knowledge\n",
    "   - Fine-tuned model: Learns SNOMED-CT code patterns\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Use More Training Data** - The paper achieves best results with 5,000 notes\n",
    "2. **Use Your Real Data** - Replace synthetic data with actual veterinary records\n",
    "3. **Train Longer** - More epochs may improve performance\n",
    "4. **Use Larger Model** - Alpaca-7B will outperform smaller models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DOWNLOAD FINE-TUNED MODEL (For Colab)\n",
    "# ============================================\n",
    "\n",
    "# Uncomment to download your fine-tuned model\n",
    "\n",
    "# import shutil\n",
    "# shutil.make_archive(\"vetllm-finetuned\", 'zip', OUTPUT_DIR)\n",
    "# \n",
    "# from google.colab import files\n",
    "# files.download('vetllm-finetuned.zip')\n",
    "\n",
    "print(\"‚úÖ Notebook complete!\")\n",
    "print(\"\\nüìÅ To download your model, uncomment and run the code above.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| **OOM Error** | Reduce `per_device_train_batch_size` to 2 or 1 |\n",
    "| **Slow Training** | Use a smaller model or reduce `num_train_epochs` |\n",
    "| **Poor Results** | Use more training data or train for more epochs |\n",
    "| **Model Not Loading** | Check `MODEL_NAME` and ensure network access |\n",
    "\n",
    "### GPU Memory Guide\n",
    "\n",
    "| Model | Required VRAM | Colab Tier |\n",
    "|-------|---------------|------------|\n",
    "| TinyLlama-1.1B | 4-6 GB | Free (T4) |\n",
    "| OpenLLaMA-3B | 8-10 GB | Free (T4) |\n",
    "| Alpaca-7B | 16-20 GB | Pro (A100) |\n",
    "\n",
    "---\n",
    "\n",
    "*Created for VetLLM Research Project*\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
